# -*- coding: utf-8 -*-
"""part_c.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_c4e-iCZZUsWTZy7JifBQJClJX8TqRIK
"""

# Imports

import numpy as np
import random
import torch
import pandas as pd
import nltk
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.metrics import f1_score, precision_score, recall_score, roc_curve, auc, precision_recall_curve
from sklearn.multiclass import OneVsRestClassifier
import matplotlib.pyplot as plt

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

english_stopwords = set(stopwords.words('english'))
seed_value = 42

def set_random_seed(seed=seed_value):
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)

def load_and_clean_data(file_path):
    data_frame = pd.read_csv(file_path, names=["category", "description"])
    data_frame.dropna(inplace=True)
    data_frame['description'] = data_frame['description'].str.replace(r'[^a-zA-Z\s]', "", regex=True).str.replace(r"\s+", " ", regex=True)
    return data_frame

# One-vs-Rest Classifier
def train_and_evaluate_classifier(model, X_train, y_train, X_val, y_val, X_test, y_test, num_classes):
    ovr_model = OneVsRestClassifier(model)
    ovr_model.fit(X_train, y_train)

    validation_metrics = {'f1_score': {}, 'precision': {}, 'recall': {}}
    test_metrics = {'f1_score': {}, 'precision': {}, 'recall': {}}
    validation_auc = {}
    test_auc = {}

    # Loop through each class to calculate metrics
    for class_label, estimator in zip(ovr_model.classes_, ovr_model.estimators_):
        print(f"Evaluating class: {class_label}")
        y_val_binary = (y_val == class_label).astype(int)
        y_test_binary = (y_test == class_label).astype(int)

        y_val_prob = estimator.decision_function(X_val)
        y_test_prob = estimator.decision_function(X_test)

        y_val_pred = estimator.predict(X_val)
        y_test_pred = estimator.predict(X_test)

        val_f1 = f1_score(y_val_binary, y_val_pred, average="macro")
        val_precision = precision_score(y_val_binary, y_val_pred, average="macro")
        val_recall = recall_score(y_val_binary, y_val_pred, average="macro")

        test_f1 = f1_score(y_test_binary, y_test_pred, average="macro")
        test_precision = precision_score(y_test_binary, y_test_pred, average="macro")
        test_recall = recall_score(y_test_binary, y_test_pred, average="macro")

        validation_metrics['f1_score'][class_label] = val_f1
        validation_metrics['precision'][class_label] = val_precision
        validation_metrics['recall'][class_label] = val_recall

        test_metrics['f1_score'][class_label] = test_f1
        test_metrics['precision'][class_label] = test_precision
        test_metrics['recall'][class_label] = test_recall

        fpr_val, tpr_val, _ = roc_curve(y_val_binary, y_val_prob)
        validation_auc[class_label] = auc(fpr_val, tpr_val)

        fpr_test, tpr_test, _ = roc_curve(y_test_binary, y_test_prob)
        test_auc[class_label] = auc(fpr_test, tpr_test)

        # ROC Curve
        plt.figure()
        plt.plot(fpr_val, tpr_val, label=f'Validation AUC (class {class_label}) = {validation_auc[class_label]:.2f}', color='lightblue')
        plt.plot(fpr_test, tpr_test, label=f'Test AUC (class {class_label}) = {test_auc[class_label]:.2f}', color='lightcoral')
        plt.plot([0, 1], [0, 1], 'k--')  # Reference line
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title(f'ROC Curve for class: {class_label}')
        plt.legend(loc="lower right")
        plt.show()

        # Precision-Recall Curve
        precision_val, recall_val, _ = precision_recall_curve(y_val_binary, y_val_prob)
        plt.figure()
        plt.plot(recall_val, precision_val, label=f'Precision-Recall Curve (Validation) for class {class_label}', color='lightgreen')
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.title(f'Precision-Recall Curve for class: {class_label}')
        plt.legend(loc="upper right")
        plt.show()


    print("\nValidation Metrics:")
    print("F1 Scores:", validation_metrics['f1_score'])
    print("Precision:", validation_metrics['precision'])
    print("Recall:", validation_metrics['recall'])

    print("\nTest Metrics:")
    print("F1 Scores:", test_metrics['f1_score'])
    print("Precision:", test_metrics['precision'])
    print("Recall:", test_metrics['recall'])

    print("\nAUC Scores for Validation Set:", validation_auc)
    print("AUC Scores for Test Set:", test_auc)

    average_val_f1 = sum(validation_metrics['f1_score'].values()) / num_classes
    average_test_f1 = sum(test_metrics['f1_score'].values()) / num_classes

    print(f"Average Validation F1 Score: {average_val_f1}")
    print(f"Average Test F1 Score: {average_test_f1}")

def main():
    set_random_seed()

    data_frame = load_and_clean_data('ecommerceDataset.csv')
    target = data_frame['category']
    num_classes = target.nunique()

    X_train, X_temp, y_train, y_temp = train_test_split(data_frame['description'], target, test_size=0.3, random_state=seed_value, stratify=target)
    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.667, random_state=seed_value, stratify=y_temp)

    tfidf_vectorizer = TfidfVectorizer()
    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
    X_val_tfidf = tfidf_vectorizer.transform(X_val)
    X_test_tfidf = tfidf_vectorizer.transform(X_test)

    model = LinearSVC(random_state=seed_value, max_iter=100, class_weight='balanced')
    print("Starting training of Logistic Regression with One-vs-Rest strategy")
    train_and_evaluate_classifier(model, X_train_tfidf, y_train, X_val_tfidf, y_val, X_test_tfidf, y_test, num_classes)


main()

